{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf643ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRanker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6b187",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0908f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = pd.read_csv('post_data_rating.csv')\n",
    "user_data = pd.read_csv('user_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcf96771",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('all_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b715166c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>action</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-27 10:31:01</td>\n",
       "      <td>488</td>\n",
       "      <td>4620</td>\n",
       "      <td>view</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-27 10:02:20</td>\n",
       "      <td>488</td>\n",
       "      <td>3608</td>\n",
       "      <td>view</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  user_id  post_id action  target\n",
       "0  2021-12-27 10:31:01      488     4620   view       1\n",
       "1  2021-12-27 10:02:20      488     3608   view       1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62ae3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.sort_values(\"timestamp\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7d551",
   "metadata": {},
   "source": [
    "The data has been preprocessed for each user while preserving class balance.\n",
    "The function below splits the dataset into train and test sets, ensuring that each user is present in both splits and that the class balance is maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "236cd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(all_data):\n",
    "    \"\"\"\n",
    "    The function splits into train/test, keeping each user in both sets and maintaining class balance.\n",
    "    \"\"\"\n",
    "    train_parts = []\n",
    "    test_parts = []\n",
    "\n",
    "    for user_id, group in all_data.groupby(\"user_id\"):\n",
    "    \n",
    "        if (group[\"target\"].sum() < 28) or ((group[\"target\"] == 0).sum() < 28):\n",
    "            continue \n",
    "\n",
    "        # split targets\n",
    "        class_1 = group[group[\"target\"] == 1]\n",
    "        class_0 = group[group[\"target\"] == 0]\n",
    "        # \n",
    "        half_1 = len(class_1) // 2\n",
    "        half_0 = len(class_0) // 2\n",
    "\n",
    "        class_1_train = class_1.iloc[:half_1]\n",
    "        class_1_test = class_1.iloc[half_1:]\n",
    "\n",
    "        class_0_train = class_0.iloc[:half_0]\n",
    "        class_0_test = class_0.iloc[half_0:]\n",
    "\n",
    "        train_parts.append(pd.concat([class_1_train, class_0_train]))\n",
    "        test_parts.append(pd.concat([class_1_test, class_0_test]))\n",
    "\n",
    "\n",
    "    train = pd.concat(train_parts).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    test = pd.concat(test_parts).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98466482",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = make_split(all_data)\n",
    "# train.to_csv('train_balanced.csv')\n",
    "# test.to_csv('test_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540eb376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tables\n",
    "train = train.merge(post_data,on='post_id',how='left')\n",
    "train = train.merge(user_data,on='user_id',how='left')\n",
    "\n",
    "test = test.merge(post_data,on='post_id',how='left')\n",
    "test = test.merge(user_data,on='user_id',how='left')\n",
    "\n",
    "train.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63049967",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_balanced.csv')\n",
    "test=pd.read_csv('test_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc542918",
   "metadata": {},
   "source": [
    "### Get random users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09586ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_users(df,n=100,seed=42):\n",
    "    rng = np.random.default_rng(seed=seed) \n",
    "    unique_users = df['user_id'].unique()\n",
    "    sample_users = rng.choice(unique_users, size=n, replace=False)\n",
    "    return sample_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61abc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = 1_000 # number of users (for quick test)\n",
    "\n",
    "sample_users = get_random_users(train,n=N_USERS,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c63ef40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[train['user_id'].isin(sample_users)]\n",
    "test_data = test[test['user_id'].isin(sample_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f993362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train\n",
    "# test_data = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375174a",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abb8eb",
   "metadata": {},
   "source": [
    "### Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53dba867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day weekday encodings rating (transform columns )\n",
    "def make_time_features(data_set):\n",
    "    data_set['timestamp'] = pd.to_datetime(data_set['timestamp'])\n",
    "    data_set['day_of_week'] = data_set.timestamp.dt.dayofweek\n",
    "    data_set['hour'] = data_set.timestamp.dt.hour\n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = make_time_features(test_data)\n",
    "train_data = make_time_features(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fe9c7",
   "metadata": {},
   "source": [
    "### Post rating feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "530f0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join post rating\n",
    "train_data = train_data.merge(post_data[['post_id','rating']],how='left',on='post_id')\n",
    "test_data = test_data.merge(post_data[['post_id','rating']],how='left',on='post_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac5e8f",
   "metadata": {},
   "source": [
    "### Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b4bb432",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_info = pd.read_csv('post_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel  \n",
    "from transformers import RobertaModel  \n",
    "from transformers import DistilBertModel  \n",
    "\n",
    "def get_model(model_name):\n",
    "    assert model_name in ['bert', 'roberta', 'distilbert']\n",
    "\n",
    "    checkpoint_names = {\n",
    "        'bert': 'bert-base-cased',  \n",
    "        'roberta': 'roberta-base', \n",
    "        'distilbert': 'distilbert-base-cased' \n",
    "    }\n",
    "\n",
    "    model_classes = {\n",
    "        'bert': BertModel,\n",
    "        'roberta': RobertaModel,\n",
    "        'distilbert': DistilBertModel\n",
    "    }\n",
    "\n",
    "    return AutoTokenizer.from_pretrained(checkpoint_names[model_name]), model_classes[model_name].from_pretrained(checkpoint_names[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_model('distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.texts['input_ids'][idx], 'attention_mask': self.texts['attention_mask'][idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts['input_ids'])\n",
    "    \n",
    "    \n",
    "dataset = PostDataset(posts_info['text'].values.tolist(), tokenizer)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, collate_fn=data_collator, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_embeddings_labels(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    total_embeddings = []\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        batch = {key: batch[key].to(device) for key in ['attention_mask', 'input_ids']}\n",
    "\n",
    "        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n",
    "\n",
    "        total_embeddings.append(embeddings.cpu())\n",
    "\n",
    "    return torch.cat(total_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings_labels(model, loader).numpy()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f805aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "centered = embeddings - embeddings.mean()\n",
    "pca = PCA(n_components=50)\n",
    "pca_decomp = pca.fit_transform(centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 15\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(pca_decomp)\n",
    "\n",
    "posts_info['TextCluster'] = kmeans.labels_\n",
    "\n",
    "dists_columns = [f'DistanceToCluster_{i}' for i in range(n_clusters)]\n",
    "\n",
    "dists_df = pd.DataFrame(\n",
    "    data=kmeans.transform(pca_decomp),\n",
    "    columns=dists_columns\n",
    ")\n",
    "\n",
    "dists_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_info = pd.concat((posts_info, dists_df), axis=1)\n",
    "posts_info.drop([\"text\"], axis=1, inplace=True)\n",
    "posts_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eecedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory \n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "del dataset\n",
    "del loader\n",
    "\n",
    "del embeddings\n",
    "del centered\n",
    "del pca\n",
    "del pca_decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f219afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_info.to_csv('post_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a79598",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(posts_info,on='post_id',how='left')\n",
    "test_data = test_data.merge(posts_info,on='post_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d422d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b8f00",
   "metadata": {},
   "source": [
    "Select features columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "191302e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['gender','age','day_of_week','hour','topic','city','os','rating',]\n",
    "features = ['gender','age','topic','country','city','os','rating']\n",
    "cat_features = ['topic','city','country','os',]\n",
    "embedding_features = ['text_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f1fc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sort_values(by='user_id') # sorting for catboosranker\n",
    "test_data = test_data.sort_values(by='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8cc75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=train_data[features],\n",
    "    label=train_data['target'],\n",
    "    group_id=train_data['user_id'],\n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=test_data[features],\n",
    "    label=test_data['target'],\n",
    "    group_id=test_data['user_id'],\n",
    "    cat_features=cat_features,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model = CatBoostRanker(\n",
    "    iterations=1000,\n",
    "    early_stopping_rounds=300,\n",
    "    loss_function='YetiRank',\n",
    "    eval_metric='NDCG:top=10',\n",
    "    custom_metric=[\n",
    "        'PrecisionAt:top=5',\n",
    "        'RecallAt:top=5',\n",
    "        'MAP:top=5'\n",
    "    ],\n",
    "    verbose=100,\n",
    "    thread_count=10,\n",
    "    random_seed=42,\n",
    "    task_type='GPU',   \n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce285912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del rank_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model.fit(train_pool, eval_set=test_pool, use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "336fb5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG:top=10;type=Base': 0.6049860733654226,\n",
       " 'PFound': 0.9023667764396479,\n",
       " 'RecallAt:top=5': 0.11906980450233327,\n",
       " 'MAP:top=5': 0.47033586052817367,\n",
       " 'PrecisionAt:top=5': 0.6022888439985761}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_model.get_best_score()['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5234140e",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b08b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model.save_model('rank_model',format='cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = rank_model.get_params()\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c85d1",
   "metadata": {},
   "source": [
    "Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model = CatBoostRanker().load_model('rank_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([train_data, test_data], axis=0)\n",
    "full_data = full_data.sort_values(by='user_id')\n",
    "\n",
    "X_full = full_data[features]\n",
    "y_full = full_data['target']\n",
    "\n",
    "group_full = full_data['user_id']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b679967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_pool = Pool(\n",
    "    data=X_full,\n",
    "    label=y_full,\n",
    "    group_id=group_full,\n",
    "    cat_features=cat_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63041d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = CatBoostRanker(**best_params)\n",
    "final_model.fit(full_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da817654",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model.save_model('rank_model_full',format='cbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c55a7",
   "metadata": {},
   "source": [
    "## Hitrate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06bdf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model = CatBoostRanker().load_model('rank_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50225a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict(df,model,n=5):\n",
    "    df = df.copy()\n",
    "    cols = model.feature_names_    \n",
    "    df['score'] = model.predict(df[cols])\n",
    "    recs = df.sort_values(by='score',ascending=False).post_id.tolist()\n",
    "    recs = recs[:n]\n",
    "    return recs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9dcc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(user_data,post_data,uid,history):\n",
    "    # take user info\n",
    "    user_row = user_data[user_data['user_id'] == uid]\n",
    "    liked_post_ids = history[(history['user_id'] == uid) & (history['target'] == 1)].post_id.tolist()\n",
    "    # remove liked on train\n",
    "    posts = post_data[~post_data['post_id'].isin(liked_post_ids)]\n",
    "    merge_features = posts.merge(user_row,how='cross')\n",
    "    return merge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32119420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_hitrate(model,train,test):\n",
    "    users_hitrate = []    \n",
    "    # user list\n",
    "    users = test[\"user_id\"].unique()    \n",
    "    for user in tqdm(users,desc='Evaluating users'):\n",
    "        # posts \n",
    "        df = get_candidates(user_data=user_data,post_data=post_data,uid=user,history=train)\n",
    "        user_records = test[(test[\"user_id\"] == user) & (test['target'] == 1)][['post_id','day_of_week','hour']] \n",
    "\n",
    "        user_hits = 0\n",
    "        for _,row in user_records.iterrows():\n",
    "            # simulate time\n",
    "            df['hour'] = row['hour']\n",
    "            df['day_of_week'] = row['hour']  \n",
    "            post_id = row['post_id']\n",
    "            recs = get_predict(df=df,model=model)        \n",
    "            # evaluate hitrate@5\n",
    "            if post_id in recs:\n",
    "                user_hits += 1                \n",
    "\n",
    "        users_hitrate.append(user_hits)\n",
    "    # hitrate per user\n",
    "    hit_rate = np.mean(users_hitrate)\n",
    "    print(sum(users_hitrate))\n",
    "    return hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "136595c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_users = get_random_users(train,n=50_000,seed=475)\n",
    "quick_test = test[test['user_id'].isin(sample_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_hit_rate = evaluate_hitrate(model=rank_model,train=train_data,test=quick_test)\n",
    "print(rank_hit_rate) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
